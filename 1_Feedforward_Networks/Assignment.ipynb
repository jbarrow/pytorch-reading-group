{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Getting Started\n",
    "\n",
    "Welcome to the Practical NLP with PyTorch reading group! The main goal of this reading group is to become comfortable using PyTorch to implement deep learning models for NLP. Each session will focus on a few different \"atoms\" -- feedforward networks, LSTMs, attention, etc. -- which are common patterns across papers.\n",
    "\n",
    "## 0.0 Readings for Next Week\n",
    "\n",
    "For next week, there will be one reading, plus this assignment:\n",
    "- for understanding **word embeddings** and **continuous bag of words (CBoW) training**, I recommend Jurafsky's [Speech and Language Processing (SLP3) Ch. 16](https://web.stanford.edu/~jurafsky/slp3/16.pdf).\n",
    "\n",
    "If there are parts of this notebook you're unfamiliar with, I'll do my best to help on Slack! In addition, I recommend the following resources:\n",
    "- the [PyTorch 60 Minute Blitz](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). We'll be going over much of this in the first meeting.\n",
    "- firstly, if you are unfamiliar with **feedforward neural networks** or **backpropagation**, I recommend no resource more than Michael Neilsen's [Neural Networks and Deep Learning Ch. 1 & 2](http://neuralnetworksanddeeplearning.com/chap1.html). It's a long read (they're 2 chapters of a book), but it is quite approachable, and very thorough.\n",
    "\n",
    "## 0.1 Feature Extraction\n",
    "\n",
    "The first step will be preprocessing the text. To make processing the data easier, we'll use scikit-learn's `CountVectorizer`, which can automatically create the bag-of-words representation from text.\n",
    "\n",
    "Below, you'll be responsible for splitting the training set into a training and validation set, as well as transforming the test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_imdb_data, data_batcher\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_raw_train, y_train = read_imdb_data('../data/aclImdb/train')\n",
    "X_raw_test, y_test = read_imdb_data('../data/aclImdb/test')\n",
    "\n",
    "# TODO: Your code goes here to create an X_raw_val set.\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_raw_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also need to transform the `X_test` and `X_val` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module):\n",
    "    \"\"\" TODO: Your code goes here. \"\"\"\n",
    "    pass\n",
    "\n",
    "    # def __init__(self, n_features, n_classes):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the classifier built, follow the instructions to train it. However, note that you'll probably want to add an inner loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the appropriate shape for the data. The number of \n",
    "# features comes from the shape of X_train.\n",
    "model = SoftmaxRegression()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, y_batch in data_(X_train, y_train):\n",
    "        # TODO: your training code goes here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you'll need to test the classifier to see how well it performed. Use the validation set to optimize the learning rate, momentum, and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Test on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Stopword Removal\n",
    "\n",
    "In order to make stopword removal easier, you can import a list of common English stopwords from the utils file. The goal here will be to create a new `X_features` vector that has all the stopwords removed. \n",
    "\n",
    "In order to accomplish this quickly, I recommend looking into the arguments of `CountVectorizer()`. Scikit-learn is amazingly comprehensive, and it's always better to not have to reinvent the wheel! (Especially when scikit-learn's wheel is a lot faster than ours would be.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stop_words\n",
    "\n",
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll then want to test if you got an improvement with stopword removal. You'll need to recreate the code above for featurizing and training the bag of words classifier, but with the correct number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit: Baselines and Bigrams\n",
    "\n",
    "Too often, not enough thought is given to our baselines before we jump right into complex deep learning models. Wang and Manning attempt to in their 2012 paper, [Baselines and Bigrams](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). It provides a **really** tough-to-beat baseline for sentiment classification.\n",
    "\n",
    "Using the paper and what you know, you should be able to implement this model in PyTorch already! You can simply use unigram features for now, though you are also welcome to recompute the feature vectors with higher-order ngrams. Thankfully, that can be done easily with just another argument to the `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NBSVM(nn.Module):\n",
    "    \"\"\" Your code goes here. \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36)",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
