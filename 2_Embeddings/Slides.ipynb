{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = '''\n",
    "it was the best of times it was the worst of times \n",
    "it was the age of wisdom it was the age of foolishness\n",
    "it was the epoch of belief it was the epoch of credulity\n",
    "'''.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(set(X))\n",
    "word_to_ix ={ word: i for i, word in enumerate(vocab) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0, 10, 6], [0, 12, 6, 8], [12, 10, 8, 4], [10, 6, 4, 0], [6, 8, 0, 12], [8, 4, 12, 9], [4, 0, 9, 6], [0, 12, 6, 8], [12, 9, 8, 4], [9, 6, 4, 0], [6, 8, 0, 12], [8, 4, 12, 11], [4, 0, 11, 6], [0, 12, 6, 3], [12, 11, 3, 4], [11, 6, 4, 0], [6, 3, 0, 12], [3, 4, 12, 11], [4, 0, 11, 6], [0, 12, 6, 5], [12, 11, 5, 4], [11, 6, 4, 0], [6, 5, 0, 12], [5, 4, 12, 7], [4, 0, 7, 6], [0, 12, 6, 2], [12, 7, 2, 4], [7, 6, 4, 0], [6, 2, 0, 12], [2, 4, 12, 7], [4, 0, 7, 6], [0, 12, 6, 1]]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "\n",
    "data = []\n",
    "X_data, y = [], []\n",
    "# look at every window in the data\n",
    "for i in range(2, len(X)-2):\n",
    "    context = [X[i - 2], X[i - 1],\n",
    "               X[i + 1], X[i + 2]]\n",
    "    target = X[i]\n",
    "    data.append((context, target))\n",
    "    X_data.append(make_context_vector(context, word_to_ix)) ; y.append(word_to_ix[target])\n",
    "\n",
    "print(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cast our data and targets as torch tensors\n",
    "X_data = torch.LongTensor(X_data)\n",
    "y = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['it', 'was', 'best', 'of'], 'the'),\n",
       " (['was', 'the', 'of', 'times'], 'best'),\n",
       " (['the', 'best', 'times', 'it'], 'of'),\n",
       " (['best', 'of', 'it', 'was'], 'times'),\n",
       " (['of', 'times', 'was', 'the'], 'it'),\n",
       " (['times', 'it', 'the', 'worst'], 'was'),\n",
       " (['it', 'was', 'worst', 'of'], 'the'),\n",
       " (['was', 'the', 'of', 'times'], 'worst'),\n",
       " (['the', 'worst', 'times', 'it'], 'of'),\n",
       " (['worst', 'of', 'it', 'was'], 'times')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.z = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = self.embeddings(x).sum(dim=1)\n",
    "        return F.log_softmax(self.z(embs), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_data(X, y, batch_size=8):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    count = 0\n",
    "    \n",
    "    while count < X.shape[0]:\n",
    "        yield X[count:(count+batch_size), :], y[count:(count+batch_size)]\n",
    "        count += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CBOW(len(vocab), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.05)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for X_batch, y_batch in batch_data(X_data, y):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_probs = model(Variable(X_batch))\n",
    "        loss = loss_function(log_probs, Variable(y_batch))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = dict(model.named_parameters())['embeddings.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = embeddings[word_to_ix['best'], :]\n",
    "w2 = embeddings[word_to_ix['worst'], :]\n",
    "\n",
    "w3 = embeddings[word_to_ix['wisdom'], :]\n",
    "w4 = embeddings[word_to_ix['foolishness'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5196\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.7928\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cos(w1, w2))\n",
    "print(cos(w3, w4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch-rg)",
   "language": "python",
   "name": "conda_pytorch-rg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
