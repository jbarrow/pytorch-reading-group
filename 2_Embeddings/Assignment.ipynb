{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import read_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "\n",
    "## 0.1 Readings\n",
    "\n",
    "The readings for next week will be:\n",
    "- [Karpathy: The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - A well-written introduction to recurrent neural networks by Andrej Karpathy (one of Fei-fei Li's former students and current head of Tesla Autopilot).\n",
    "- [Chris Olah: Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) - An introduction to the computations that go on inside an LSTM, written by Chris Olah (Google Brain, and creator of interactive research journal [distill.pub](http://distill.pub)).\n",
    "\n",
    "A good resource for today's task (besides the assigned Jurafsky readings from last week) is the following tutorial: [Word2Vec Tutorial: The SkipGram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). Part 2 provides a really good overview of negative sampling!\n",
    "\n",
    "## 0.2 Task\n",
    "\n",
    "This week your tasks will be:\n",
    "1. using off the shelf embeddings to \n",
    "2. training a neural network on the Skipgram task to create word embeddings. If you're accurate enough, you can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Off-the-Shelf Embeddings\n",
    "\n",
    "The first task is to use off-the-shelf embeddings to reattempt the IMDB classification. The function below reads the embeddings from a text file and returns the vectors as a `np.array`, as well as `word_to_ix`, which is a `dict` that maps from word to the index of the row in the array and `ix_to_word`, which is a list that maps from the index in the array to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, word_to_ix, ix_to_word = read_embeddings('../data/glove.6B.50d.txt', length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7644\n",
      "[ -5.12440000e-01   1.37550000e+00  -1.02030000e+00  -1.61290000e-01\n",
      "   4.63910000e-01   5.20210000e-01  -1.25540000e-01  -9.24370000e-01\n",
      "  -2.89470000e-01  -1.43390000e-01   3.35830000e-01   2.51460000e-01\n",
      "   1.02190000e+00  -1.28130000e-01  -3.98560000e-01  -7.64740000e-02\n",
      "  -6.97520000e-01   2.09050000e-01  -9.28610000e-01  -9.80310000e-01\n",
      "  -1.01630000e+00  -5.03380000e-01   1.10990000e+00  -1.04600000e+00\n",
      "  -8.72510000e-01  -4.71210000e-01  -8.33200000e-01   1.74180000e+00\n",
      "   4.39090000e-01  -1.20890000e+00   1.46100000e+00   3.15650000e-01\n",
      "  -3.03300000e-01   3.27280000e-02  -1.77280000e-01   4.93680000e-01\n",
      "  -2.78910000e-03  -3.54150000e-01  -2.78760000e-01  -6.22390000e-01\n",
      "   9.43790000e-02   1.72130000e-03  -6.85390000e-01  -8.15770000e-01\n",
      "   1.00790000e+00  -2.43800000e-01  -2.20430000e-03  -1.40590000e+00\n",
      "  -4.30230000e-01  -5.63840000e-01]\n",
      "purple\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix['purple'])\n",
    "print(vectors[word_to_ix['purple'], :])\n",
    "print(ix_to_word[7644])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to featurize all the documents, you'll be averaging the word vectors of all the words in the documents. There are two ways to tackle this problem:\n",
    "\n",
    "1. an iterative solution, using a `for` loop over documents and then a `for` loop over words, to sum, count, and divide\n",
    "2. a matrix-based solution (which is much faster)\n",
    "\n",
    "To arrive at the matrix solution, we'll be trying to compute a document matrix $D$, for which every row is the feature vector of the document. So, if we have 25,000 documents and 300-dimensional word embeddings, $D$ is a $25,000$x$300$ element matrix. We have an array existing word vectors $W$, which in this case we'll say is $|V|$x$300$. What we're missing is a $25,000$x$|V|$ matrix that contains all the words in each document. What's an efficient way to get that?\n",
    "\n",
    "It turns out we already know one! Scikit-learn's `CountVectorizer` returns exactly that! Note that **you'll need to pass in the existing vocabulary so everything works smoothly**, but that's a parameter you can initialize the `CountVectorizer` with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have constructed the document matrix, you can classify each of the rows using softmax regression. You're welcome to use either your previous implementation or the existing scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Up to you if you use the following or not:\n",
    "#\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Skipgrams\n",
    "\n",
    "The second task is to train a SkipGram model. Take inspiration from the CBoW classifier trained in the slides, but remember the implementational differences in skipgram.\n",
    "\n",
    "For one, you generally think about it as having 2 embedding matrices, a word-embedding and a context-embedding. Secondly, you're predicting the context from the words, not the other way around. That part won't change your model code, but it will change how you preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = '''\n",
    "it was the best of times it was the worst of times \n",
    "it was the age of wisdom it was the age of foolishness\n",
    "it was the epoch of belief it was the epoch of credulity\n",
    "'''.split()\n",
    "\n",
    "# You'll need to write code that takes in the dataset and creates \n",
    "# the training examples from it. Remember, the training examples\n",
    "# are (w, c) pairs for every word c in the context window of word w.\n",
    "\n",
    "# for instance, on the training set above, the pairs for w = best, and a \n",
    "# will be:\n",
    "#\n",
    "# (best, was)\n",
    "# (best, the)\n",
    "# (best, of)\n",
    "# (best, times)\n",
    "def make_examples(X, context_size=2):\n",
    "    # Your code goes here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \"\"\" Your code goes here. \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've created the model, you'll need to train it. I've provided you with a batch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_data(X, y, batch_size=8):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    count = 0\n",
    "    \n",
    "    while count < X.shape[0]:\n",
    "        yield X[count:(count+batch_size), :], y[count:(count+batch_size)]\n",
    "        count += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "\n",
    "# Your code goes here.\n",
    "# Remember to instantiate a model, loss function, and optimizer\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Your code goes here.\n",
    "    # You should use the batch_data function, but remember to wrap your batches\n",
    "    # in a Variable()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit: Negative Sampling\n",
    "\n",
    "Finally, for those of you who want to take on the challenge, I encourage you to attempt to implement negative sampling! Your two options are to write a general purpose negative sampling loss module, or to wrap the whole thing up into the implementation of Skipgram.\n",
    "\n",
    "The first step will be using a larger dataset to train the vectors on. We'll use the IMDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import read_imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = read_imdb_data('../data/aclImdb/test')\n",
    "X = ' '.join(X).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can limit the size of the vocabulary to speed up training as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X = [w for w in X if w in vectorizer.vocabulary_.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go through the same process you went through above to generate the training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NegativeSamplingSkipgram(nn.Module):\n",
    "    \"\"\" Your code goes here. \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "\n",
    "# Your code goes here.\n",
    "# Remember to instantiate a model, loss function, and optimizer\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Your code goes here.\n",
    "    # You should use the batch_data function, but remember to wrap your batches\n",
    "    # in a Variable()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch-rg)",
   "language": "python",
   "name": "conda_pytorch-rg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
